---
title: Tokenisation dans Code2Prompt
description: D√©couvrez la tokenisation et comment Code2Prompt traite le texte pour les LLMs.
---

Lorsque l'on travaille avec des mod√®les de langage, le texte doit √™tre transform√© en un format que le mod√®le peut comprendre : des **jetons** (tokens), qui sont des s√©quences de nombres. Cette transformation est g√©r√©e par un **tokeniseur**.

---

## Qu'est-ce qu'un Tokeniseur ?

Un tokeniseur convertit le texte brut en jetons, qui sont les briques de base pour la fa√ßon dont les mod√®les de langage traitent les entr√©es. Ces jetons peuvent repr√©senter des mots, des sous-mots ou m√™me des caract√®res individuels, en fonction de la conception du tokeniseur.

Pour `code2prompt`, nous utilisons le tokeniseur **tiktoken**. Il est efficace, robuste et optimis√© pour les mod√®les OpenAI.
Vous pouvez explorer ses fonctionnalit√©s dans le r√©f√©rentiel officiel

üëâ [R√©f√©rentiel GitHub de tiktoken](https://github.com/openai/tiktoken)

Si vous souhaitez en savoir plus sur les tokeniseurs en g√©n√©ral, consultez le

üëâ [Guide de tokenisation de Mistral](https://docs.mistral.ai/guides/tokenization/).

## Impl√©mentation dans `code2prompt`

La tokenisation est impl√©ment√©e √† l'aide de [`tiktoken-rs`](https://github.com/zurawiki/tiktoken-rs). `tiktoken` prend en charge ces codages utilis√©s par les mod√®les OpenAI :

| Argument CLI | Nom de codage           | Mod√®les OpenAI                                                             |
| ---- | ----------------------- | ------------------------------------------------------------------------- |
| `cl100k` | `cl100k_base`           | Mod√®les ChatGPT, `text-embedding-ada-002`                                  |
| `p50k` | `p50k_base`             | Mod√®les de code, `text-davinci-002`, `text-davinci-003`                       |
| `p50k_edit` | `p50k_edit`             | Utiliser pour les mod√®les d'√©dition comme `text-davinci-edit-001`, `code-davinci-edit-001` |
| `r50k` | `r50k_base` (ou `gpt2`) | Mod√®les GPT-3 comme `davinci`                                               |
| `gpt2` | `o200k_base`            | Mod√®les GPT-4o                                                             |

Pour plus de contexte sur les diff√©rents tokeniseurs, consultez le [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/66b988407d8d13cad5060a881dc8c892141f2d5c/examples/How_to_count_tokens_with_tiktoken.ipynb)

Note that I kept code blocks, commands, and variable names unchanged as per your request. I also adjusted no relative paths as there were none starting with "../".

Please let me know if I can help with anything else.

Also, I used French style and grammar guidelines from the "Rectifications de l'orthographe du fran√ßais en 1990" and "Typographie fran√ßaise". If any changes are required according to your organization's style guide, please provide it.
