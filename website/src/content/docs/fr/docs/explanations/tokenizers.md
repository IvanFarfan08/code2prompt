---
title: Tokenisation dans Code2Prompt
description: D√©couvrez la tokenisation et comment Code2Prompt traite le texte pour les LLMs.
---

Lorsque l'on travaille avec des mod√®les de langage, le texte doit √™tre transform√© en un format que le mod√®le peut comprendre ‚Äî **tokens**, qui sont des s√©quences de nombres. Cette transformation est g√©r√©e par un **tokeniseur**.

---

## Qu'est-ce qu'un Tokeniseur ?

Un tokeniseur convertit le texte brut en tokens, qui sont les blocs de construction pour la fa√ßon dont les mod√®les de langage traitent l'entr√©e. Ces tokens peuvent repr√©senter des mots, des sous-mots ou m√™me des caract√®res individuels, selon la conception du tokeniseur.

Pour `code2prompt`, nous utilisons le tokeniseur **tiktoken**. Il est efficace, robuste et optimis√© pour les mod√®les OpenAI.
Vous pouvez explorer sa fonctionnalit√© dans le r√©f√©rentiel officiel

üëâ [R√©f√©rentiel GitHub de tiktoken](https://github.com/openai/tiktoken)

Si vous souhaitez en savoir plus sur les tokeniseurs en g√©n√©ral, consultez le

üëâ [Guide de tokenisation Mistral](https://docs.mistral.ai/guides/tokenization/).

## Impl√©mentation dans `code2prompt`

La tokenisation est impl√©ment√©e √† l'aide de [`tiktoken-rs`](https://github.com/zurawiki/tiktoken-rs). `tiktoken` prend en charge ces encodages utilis√©s par les mod√®les OpenAI :

| Argument CLI | Nom de l'encodage       | Mod√®les OpenAI                                                           |
| ---- | ----------------------- | ----------------------------------------------------------------------- |
| `cl100k` | `cl100k_base`           | Mod√®les ChatGPT, `text-embedding-ada-002`                                |
| `p50k` | `p50k_base`             | Mod√®les de code, `text-davinci-002`, `text-davinci-003`                   |
| `p50k_edit` | `p50k_edit`             | Utiliser pour les mod√®les d'√©dition comme `text-davinci-edit-001`, `code-davinci-edit-001` |
| `r50k` | `r50k_base` (ou `gpt2`) | Mod√®les GPT-3 comme `davinci`                                             |
| `gpt2` | `o200k_base`            | Mod√®les GPT-4o                                                           |

Pour plus de contexte sur les diff√©rents tokeniseurs, consultez le [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/66b988407d8d13cad5060a881dc8c892141f2d5c/examples/How_to_count_tokens_with_tiktoken.ipynb)

> Cette page a √©t√© traduite automatiquement pour votre commodit√©. Veuillez vous r√©f√©rer √† la version anglaise pour le contenu original.
