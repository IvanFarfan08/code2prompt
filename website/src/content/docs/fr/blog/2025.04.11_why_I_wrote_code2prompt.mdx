---
title: "Pourquoi j'ai d√©velopp√© Code2Prompt"
date: 2025-04-11
lastUpdated: 2025-04-11
tags:
  - open-source
  - code2prompt
  - IA
  - Agent
excerpt: "L'histoire derri√®re code2prompt : ma qu√™te Open-Source pour relever les d√©fis de contexte dans les flux de travail LLM"
authors:
  - ODAncona
cover:
  alt: "Une illustration de code2prompt simplifiant le contexte de code pour les agents IA."
  image: "/src/assets/logo_dark_v0.0.2.svg"
featured: false
draft: false
---

## Introduction

Je suis toujours fascin√© par la fa√ßon dont les mod√®les de langage √† grande √©chelle (LLM) transforment les flux de travail de codage - g√©n√©rant des tests, des docstrings ou m√™me des fonctionnalit√©s enti√®res en quelques minutes. Mais √† mesure que je poussais ces mod√®les plus loin, quelques points de douleur critiques continuaient √† √©merger :

| Difficult√©s de planification | Co√ªts de jetons √©lev√©s | Hallucinations |
| ---------------------------- | ---------------------- | -------------- |
| üß† ‚û°Ô∏è ü§Ø                     | üî• ‚û°Ô∏è üí∏               | üí¨ ‚û°Ô∏è üåÄ       |

C'est pourquoi j'ai commenc√© √† contribuer √† `code2prompt`, un outil bas√© sur Rust pour aider √† fournir juste le contexte appropri√© aux LLM.

Dans cet article, je partagerai mon parcours et expliquerai pourquoi je suis convaincu que `code2prompt` est pertinent aujourd'hui et s'int√®gre si bien, et pourquoi il est devenu ma solution incontournable pour des flux de travail de codage IA meilleurs et plus rapides.

## Mes premiers pas avec les LLM üë£

J'ai commenc√© √† exp√©rimenter avec les LLM sur `OpenAI Playground` avec `text-davinci-003` lorsqu'il a gagn√© en popularit√© en novembre 2023. Les mod√®les de langage ont permis une nouvelle r√©volution. Cela ressemblait √† avoir un assistant brillant qui pouvait produire des tests unitaires et des docstrings presque sur commande. J'ai appr√©ci√© pousser les mod√®les √† leurs limites - testant tout, des conversations informelles et des dilemmes √©thiques aux jailbreaks et aux t√¢ches de codage complexes. Cependant, √† mesure que j'ai abord√© des projets plus importants, j'ai rapidement r√©alis√© que les mod√®les avaient des limitations criantes. Au d√©but, je ne pouvais adapter que quelques centaines de lignes de code dans la fen√™tre de contexte, et m√™me alors, les mod√®les avaient souvent du mal √† comprendre le but ou la structure du code. C'est pourquoi j'ai rapidement remarqu√© que l'importance du contexte √©tait primordiale. Plus mes instructions √©taient concises et meilleur √©tait le contexte, meilleurs √©taient les r√©sultats.

![OpenAI Playground](/assets/blog/post1/playground.png)

## √âvolution des mod√®les üèóÔ∏è

Les mod√®les pouvaient produire des r√©sultats impressionnants mais avaient souvent du mal avec des bases de code plus importantes ou des t√¢ches complexes. Je me suis retrouv√© √† passer plus de temps √† √©laborer des invites qu'√† coder r√©ellement. Dans le m√™me temps, les mod√®les continuaient √† s'am√©liorer avec la sortie de nouvelles versions. Ils ont augment√© leurs capacit√©s de raisonnement et la taille du contexte, offrant de nouvelles perspectives et possibilit√©s. Je pouvais adapter presque deux mille lignes de code dans la fen√™tre de contexte, et les r√©sultats se sont am√©lior√©s. Je pouvais √©crire des fonctionnalit√©s enti√®res en quelques it√©rations, et j'ai √©t√© impressionn√© par la rapidit√© avec laquelle je pouvais obtenir des r√©sultats. J'√©tais convaincu que les LLM √©taient l'avenir du codage, et je voulais en faire partie. Je crois fermement que l'IA ne nous remplacera pas encore. Mais nous assistera sous la forme d'assistants o√π les humains sont les experts encore en contr√¥le.

## Mes premiers projets avec les LLM üöÄ

J'ai commenc√© √† √©crire un module de recherche de chemin `ROS` pour un concours de robotique, √† g√©n√©rer des fonctionnalit√©s pour une application `Flutter` multiplateforme d'architecture propre, et j'ai cr√©√© une petite application Web pour suivre mes d√©penses en `Next.js`. Le fait que j'ai construit cette petite application en une soir√©e, dans un framework que je n'avais jamais utilis√© auparavant, a √©t√© un moment d√©cisif pour moi ; les LLM n'√©taient pas seulement des outils mais des multiplicateurs. J'ai d√©velopp√© `bboxconverter`, un package pour convertir des bo√Ætes de bounding, et la liste continue. Les LLM peuvent vous aider √† apprendre de nouvelles technologies et frameworks rapidement ; c'est incroyable.

## Un nouveau paradigme : Software 3.0 üí°

Je me suis approfondi dans les LLM et j'ai commenc√© √† construire des agents et des squelettes autour d'eux. J'ai reproduit le c√©l√®bre article [RestGPT](https://restgpt.github.io/). L'id√©e est excellente : donner aux LLM la capacit√© d'appeler certaines API REST avec une sp√©cification OpenAPI, telles que `Spotify` ou `TMDB`. Ces capacit√©s introduisent un nouveau paradigme de programmation logiciel, que j'aime appeler **Software 3.0**.

| Software 1.0        | Software 2.0           | Software 3.0 |
| ------------------- | ---------------------- | ------------ |
| Bas√© sur des r√®gles | Pilot√© par les donn√©es | Agence       |

La m√™me id√©e a propuls√© le protocole [MCP](https://modelcontextprotocol.io/introduction), qui permet aux LLM d'appeler des outils et des ressources directement de mani√®re transparente, car par conception, l'outil a besoin d'une description pour √™tre appel√© par le LLM, contrairement aux API REST qui ne n√©cessitent pas n√©cessairement de sp√©cification OpenAPI.

## Les limitations des LLM üß©

### Hallucinations üåÄ

Lors de la reproduction du c√©l√®bre article `RESTGPT`, j'ai remarqu√© certaines limitations graves des LLM. Les auteurs de l'article ont rencontr√© les m√™mes probl√®mes que moi : les LLM **hallucinaient**. Ils g√©n√®rent du code qui n'est pas impl√©ment√©, inventant des arguments et suivant simplement les instructions √† la lettre sans utiliser le bon sens. Par exemple, dans le code source original de RestGPT, les auteurs ont demand√© dans [l'invite de l'appelant](https://github.com/Yifan-Song793/RestGPT/blob/main/model/caller.py).

> "de ne pas √™tre malin et d'inventer des √©tapes qui n'existent pas dans le plan."

J'ai trouv√© cette d√©claration amusante et tr√®s int√©ressante parce que c'√©tait la premi√®re fois que je rencontrais quelqu'un qui instruisait les LLM √† ne pas halluciner.

### Taille de contexte limit√©e üìè

Une autre limitation √©tait la taille du contexte ; les LLM performent bien pour trouver l'aiguille dans la botte de foin mais ont du mal √† en comprendre le sens. Lorsque vous donnez trop de contexte aux mod√®les de langage, ils ont tendance √† se perdre dans les d√©tails et √† perdre de vue l'ensemble, ce qui est ennuyeux et n√©cessite une direction constante. La fa√ßon dont j'aime y penser est similaire √† [la mal√©diction de la dimensionnalit√©](https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb/). Remplacez le mot "dimension" ou "fonctionnalit√©" par "contexte", et vous obtenez l'id√©e.

![Mal√©diction de la dimensionnalit√©](/assets/blog/post1/curse_of_dimensionality.png)

Plus vous donnez de contexte au LLM, plus il est difficile de trouver la bonne r√©ponse. J'ai cr√©√© une phrase agr√©able pour r√©sumer cette id√©e :

> Fournissez aussi peu de contexte que possible mais autant que n√©cessaire

Ceci est fortement inspir√© par la c√©l√®bre [citation d'Alain Berset](https://www.lematin.ch/story/alain-berset-la-formule-qui-defie-le-temps-166189802108), un politicien suisse üá®üá≠ qui a d√©clar√© pendant le confinement COVID-19 :

> "Nous souhaitons agir aussi vite que possible, mais aussi lentement que n√©cessaire"

Cela repr√©sente l'id√©e de compromis et s'applique √† la taille du contexte des LLM !

## Recherche d'une meilleure fa√ßon : code2prompt üî®

Par cons√©quent, j'avais besoin d'un moyen de charger, de filtrer et d'organiser rapidement mon contexte de code en fournissant la moindre quantit√© possible de contexte avec la meilleure qualit√© possible. J'ai essay√© de copier manuellement des fichiers ou des extraits dans des invites, mais cela est devenu encombrant et sujet aux erreurs. Je savais que l'automatisation du processus fastidieux de forgeage du contexte pour poser de meilleures questions serait utile. Ensuite, un jour, j'ai tap√© "code2prompt" sur Google, esp√©rant trouver un outil qui acheminait mon code directement dans des invites.

Et voici ! J'ai d√©couvert un projet **bas√© sur Rust** de [Mufeed](https://www.reddit.com/r/rust/comments/1bghroh/i_made_code2prompt_a_cli_tool_to_convert_your/) nomm√© _code2prompt_, qui comptait environ 200 √©toiles sur GitHub. C'√©tait encore basique √† l'√©poque : un simple outil CLI avec une capacit√© de filtration limit√©e et des mod√®les. J'ai vu un √©norme potentiel et j'ai saut√© directement pour contribuer, en mettant en ≈ìuvre la correspondance de mod√®les globaux, entre autres fonctionnalit√©s, et je suis rapidement devenu le principal contributeur.

## Vision & Int√©grations üîÆ

Aujourd'hui, il existe plusieurs fa√ßons de fournir du contexte aux LLM. G√©n√©rer √† partir du contexte plus large, utiliser la g√©n√©ration augment√©e de r√©cup√©ration (RAG), [compresser le code](https://www.all-hands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents), ou m√™me utiliser une combinaison de ces m√©thodes. Le forgeage de contexte est un sujet br√ªlant qui √©voluera rapidement dans les prochains mois. Cependant, mon approche est **KISS** : Keep It Simple, Stupid. La meilleure fa√ßon de fournir du contexte aux LLM est d'utiliser la fa√ßon la plus simple et la plus efficace possible. Vous forgez pr√©cis√©ment le contexte dont vous avez besoin ; c'est d√©terministe, contrairement √† RAG.

C'est pourquoi j'ai d√©cid√© de pousser `code2prompt` plus loin en tant qu'outil simple pouvant √™tre utilis√© dans n'importe quel flux de travail. Je voulais le rendre facile √† utiliser, facile √† int√©grer et facile √† √©tendre. C'est pourquoi j'ai ajout√© de nouvelles fa√ßons d'interagir avec l'outil.

- **Core** : Le c≈ìur de `code2prompt` est une biblioth√®que Rust qui fournit la fonctionnalit√© de base pour forger le contexte √† partir de votre base de code. Il comprend une API simple pour charger, filtrer et organiser votre contexte de code.
- **CLI** : L'interface de ligne de commande est la fa√ßon la plus simple d'utiliser `code2prompt`. Vous pouvez forger le contexte √† partir de votre base de code et le canaliser directement dans vos invites.
- **API Python** : L'API Python est un simple wrapper autour de CLI qui vous permet d'utiliser `code2prompt` dans vos scripts et agents Python. Vous pouvez forger le contexte √† partir de votre base de code et le canaliser directement dans vos invites.
- **MCP** : Le serveur MCP `code2prompt` permet aux LLM d'utiliser `code2prompt` en tant qu'outil, les rendant ainsi capables de forger le contexte.

La vision est d√©crite plus en d√©tail dans la [page de vision](/docs/vision) de la documentation.

## Int√©gration avec les agents üë§

Je crois que les futurs agents auront besoin d'un moyen d'ing√©rer du contexte, et `code2prompt` est la fa√ßon simple et efficace de le faire pour les r√©f√©rentiels textuels comme la base de code, la documentation ou les notes. Un endroit typique pour utiliser `code2prompt` serait dans une base de code avec des conventions de d√©nomination significatives. Par exemple, dans l'architecture propre, il existe une s√©paration claire des pr√©occupations et des couches. Le contexte pertinent r√©side g√©n√©ralement dans diff√©rents fichiers et dossiers mais partage le m√™me nom. C'est un cas d'utilisation parfait pour `code2prompt`, o√π vous pouvez utiliser le mod√®le global pour saisir les fichiers pertinents.

**Bas√© sur le mod√®le global** : S√©lectionnez ou excluez pr√©cis√©ment les fichiers avec un minimum de tracas.

En outre, la biblioth√®que principale est con√ßue en tant que gestionnaire de contexte √©tatique, vous permettant d'ajouter ou de supprimer des fichiers √† mesure que votre conversation avec le LLM √©volue. Ceci est particuli√®rement utile pour fournir du contexte pour une t√¢che ou un objectif sp√©cifique. Vous pouvez facilement ajouter ou supprimer des fichiers du contexte sans relancer le processus.

**Contexte √©tatique** : Ajoutez ou supprimez des fichiers √† mesure que votre conversation avec le LLM √©volue.

Ces capacit√©s font de `code2prompt` un choix parfait pour les flux de travail bas√©s sur des agents. Le serveur MCP permet une int√©gration transparente avec des frameworks d'agents IA populaires tels que [Aider](https://github.com/paul-gauthier/aider), [Goose](https://block.github.io/goose/), ou [Cline](https://github.com/jhillyerd/cline). Laissez-les g√©rer des objectifs complexes pendant que `code2prompt` fournit le contexte de code parfait.

## Pourquoi Code2prompt compte ‚úä

√Ä mesure que les LLM √©voluent et que les fen√™tres de contexte s'√©tendent, il peut sembler que simplement forcer des r√©f√©rentiels entiers dans des invites suffit. Cependant, les **co√ªts de jetons** et la **coh√©rence des invites** restent des obstacles importants pour les petites entreprises et les d√©veloppeurs. En se concentrant sur le code qui compte, `code2prompt` maintient votre utilisation de LLM efficace, rentable et moins encline √† l'hallucination.

**En bref :**

- **R√©duire les hallucinations** en fournissant la bonne quantit√© de contexte
- **R√©duire les co√ªts de jetons** en curant manuellement le contexte appropri√© n√©cessaire
- **Am√©liorer les performances de LLM** en donnant la bonne quantit√© de contexte
- Int√®gre la pile agence en tant que fournisseur de contexte pour les r√©f√©rentiels textuels

## Vous pouvez rejoindre ! C'est Open Source ! üåê

Tout nouveau contributeur est le bienvenu ! Venez √† bord si vous √™tes int√©ress√© par Rust, la cr√©ation d'outils IA innovants, ou si vous voulez simplement un meilleur flux de travail pour vos invites bas√©es sur le code.

Merci de lire, et j'esp√®re que mon histoire vous a inspir√© √† d√©couvrir code2prompt. C'est un incroyable voyage, et cela ne fait que commencer !

**Olivier D'Ancona**

> Cette page a √©t√© traduite automatiquement pour votre commodit√©. Veuillez vous r√©f√©rer √† la version anglaise pour le contenu original.
