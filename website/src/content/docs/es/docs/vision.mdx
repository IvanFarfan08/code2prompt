---
title: La visi√≥n de Code2Prompt
description: Descubre la visi√≥n detr√°s de Code2Prompt y c√≥mo mejora las interacciones de LLM con el c√≥digo.
---

import { Card } from "@astrojs/starlight/components";
import { Aside } from "@astrojs/starlight/components";

<Card title="Prop√≥sito üéØ">
  `code2prompt` fue creado para ayudar a los desarrolladores y agentes de
  inteligencia artificial a interactuar con bases de c√≥digo de manera m√°s
  efectiva.
</Card>

## El problema üö©

Los modelos de lenguaje grandes (LLM) han revolucionado la forma en que interactuamos con el c√≥digo. Sin embargo, todav√≠a enfrentan desaf√≠os significativos con la generaci√≥n de c√≥digo:

- **Planificaci√≥n y razonamiento**: Los LLM carecen de la capacidad de planificar y razonar, lo cual es crucial para tareas como la generaci√≥n de c√≥digo, la refactorizaci√≥n y la depuraci√≥n. A menudo luchan por obtener una visi√≥n general y son cortoplacistas.
- **Tama√±o del contexto**: Los LLM tienen una ventana de contexto limitada, lo que restringe su capacidad para analizar y comprender bases de c√≥digo grandes.
- **Alucinaci√≥n**: Los LLM pueden generar c√≥digo que parece correcto pero que en realidad es incorrecto o absurdo. Este fen√≥meno, conocido como alucinaci√≥n, ocurre cuando el modelo carece de contexto suficiente o comprensi√≥n de la base de c√≥digo.

Aqu√≠ es donde entra en juego `code2prompt`.

## La soluci√≥n ‚úÖ

Creemos que la planificaci√≥n y el razonamiento pueden lograrse mediante t√©cnicas de andamiaje con agentes humanos o de inteligencia artificial. Estos agentes necesitan recopilar un **contexto de alta calidad** de la base de c√≥digo que est√© filtrado, estructurado y formateado para la tarea en cuesti√≥n.

La regla general ser√≠a:

<Aside type="tip">
  > proporcionar la menor cantidad de contexto posible, pero la necesaria
</Aside>

Esto es pr√°cticamente dif√≠cil de lograr, especialmente para bases de c√≥digo grandes. Sin embargo, `code2prompt` es una herramienta simple que puede ayudar a los desarrolladores y agentes de inteligencia artificial a ingerir la base de c√≥digo de manera m√°s efectiva.

Automatiza el proceso de recorrer una base de c√≥digo, filtrar archivos y formatearlos en indicaciones estructuradas que los LLM pueden comprender. Al hacerlo, ayuda a mitigar los desaf√≠os de planificaci√≥n, razonamiento y alucinaci√≥n.

Puede entender c√≥mo `code2prompt` est√° dise√±ado para abordar estos desaf√≠os en la siguiente secci√≥n.

## Arquitectura ‚õ©Ô∏è

<img
  src="/assets/images/architecture.svg"
  alt="Arquitectura de code2prompt"
  style="width: 75%;"
/>

`code2prompt` est√° dise√±ado de manera modular, lo que permite una f√°cil integraci√≥n en varios flujos de trabajo. Puede utilizarse como una biblioteca central, una interfaz de l√≠nea de comandos (CLI), un kit de desarrollo de software (SDK) o incluso como un servidor de protocolo de contexto de modelo (MCP).

### Central

`code2prompt` es una herramienta de ingesta de c√≥digo que agiliza el proceso de crear indicaciones de LLM para an√°lisis de c√≥digo, generaci√≥n y otras tareas. Funciona recorriendo directorios, construyendo una estructura de √°rbol y recopilando informaci√≥n sobre cada archivo. La biblioteca central se puede integrar f√°cilmente en otras aplicaciones.

### CLI

La interfaz de l√≠nea de comandos (CLI) de `code2prompt` fue dise√±ada para que los humanos generen indicaciones directamente desde su base de c√≥digo. La indicaci√≥n generada se copia autom√°ticamente al portapapeles y tambi√©n se puede guardar en un archivo de salida. Adem√°s, puede personalizar la generaci√≥n de indicaciones utilizando plantillas de Handlebars. ¬°Eche un vistazo a las indicaciones proporcionadas en la documentaci√≥n!

### SDK

El kit de desarrollo de software (SDK) de `code2prompt` ofrece una vinculaci√≥n de Python a la biblioteca central. Esto es perfecto para agentes de inteligencia artificial o scripts de automatizaci√≥n que desean interactuar con la base de c√≥digo sin problemas. El SDK se hospeda en Pypi y se puede instalar mediante pip.

### MCP

`code2prompt` tambi√©n est√° disponible como un servidor de protocolo de contexto de modelo (MCP), lo que permite ejecutarlo como un servicio local. Esto permite a los LLM en esteroides proporcionarles una herramienta para recopilar autom√°ticamente un contexto bien estructurado de su base de c√≥digo.

> Esta p√°gina ha sido traducida autom√°ticamente para su conveniencia. Consulte la versi√≥n en ingl√©s para ver el contenido original.
