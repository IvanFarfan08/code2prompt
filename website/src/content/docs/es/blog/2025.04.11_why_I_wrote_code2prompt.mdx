---
title: "Por qu√© desarroll√© Code2Prompt"
date: 2025-04-11
lastUpdated: 2025-04-11
tags:
  - open-source
  - code2prompt
  - AI
  - Agent
excerpt: "La historia detr√°s de code2prompt: mi b√∫squeda de c√≥digo abierto para abordar los desaf√≠os de contexto en los flujos de trabajo de LLM"
authors:
  - ODAncona
cover:
  alt: "Una ilustraci√≥n de code2prompt que agiliza el contexto del c√≥digo para agentes de inteligencia artificial."
  image: "/src/assets/logo_dark_v0.0.2.svg"
featured: false
draft: false
---

## Introducci√≥n

Siempre me ha fascinado c√≥mo los modelos de lenguaje grandes (LLM) transforman los flujos de trabajo de codificaci√≥n, generando pruebas, docstrings o incluso enviando caracter√≠sticas completas en minutos. Pero a medida que empujaba a estos modelos m√°s lejos, surg√≠an algunos puntos cr√≠ticos:

| Dificultades de planificaci√≥n | Altos costos de tokens | Alucinaciones |
| ----------------------------- | ---------------------- | ------------- |
| üß† ‚û°Ô∏è ü§Ø                      | üî• ‚û°Ô∏è üí∏               | üí¨ ‚û°Ô∏è üåÄ      |

Es por eso que comenc√© a contribuir a `code2prompt`, una herramienta basada en Rust para ayudar a proporcionar el contexto adecuado a los LLM.

En este post, compartir√© mi viaje y explicar√© por qu√© estoy convencido de que `code2prompt` es relevante hoy en d√≠a y se integra tan bien, y por qu√© se ha convertido en mi soluci√≥n para flujos de trabajo de codificaci√≥n con inteligencia artificial m√°s r√°pidos y mejores.

## Mis primeros pasos con LLM üë£

Comenc√© a experimentar con LLM en `OpenAI Playground` con `text-davinci-003` cuando gan√≥ popularidad en noviembre de 2023. Los modelos de lenguaje permitieron una nueva revoluci√≥n. Se sinti√≥ como tener un asistente brillante que generaba pruebas unitarias y docstrings casi a pedido. Disfrut√© empujando a los modelos a sus l√≠mites, probando todo, desde charlas peque√±as y dilemas √©ticos hasta jailbreaks y tareas de codificaci√≥n complejas. Sin embargo, a medida que asum√≠ proyectos m√°s extensos, r√°pidamente me di cuenta de que los modelos ten√≠an limitaciones evidentes. Al principio, solo pod√≠a ajustar unas pocas cientos de l√≠neas de c√≥digo en la ventana de contexto, y ni siquiera entonces, los modelos a menudo luchaban por comprender el prop√≥sito o la estructura del c√≥digo. Es por eso que r√°pidamente not√© que la importancia del contexto era fundamental. Cuanto m√°s concisas eran mis instrucciones y mejor era el contexto, mejores eran los resultados.

![OpenAI Playground](/assets/blog/post1/playground.png)

## Evoluci√≥n del modelo üèóÔ∏è

Los modelos pod√≠an producir resultados impresionantes, pero a menudo luchaban con bases de c√≥digo m√°s grandes o tareas complejas. Me encontr√© pasando m√°s tiempo elaborando indicaciones que codificando. Al mismo tiempo, los modelos segu√≠an mejorando con el lanzamiento de nuevas versiones. Aumentaron las habilidades de razonamiento y el tama√±o del contexto, ofreciendo nuevas perspectivas y posibilidades. Pude ajustar casi dos mil l√≠neas de c√≥digo en la ventana de contexto, y los resultados mejoraron. Pude escribir caracter√≠sticas completas en cuesti√≥n de unas pocas iteraciones, y me asombr√© de lo r√°pido que pod√≠a obtener resultados. Estaba convencido de que los LLM eran el futuro de la codificaci√≥n, y quer√≠a ser parte de esa revoluci√≥n. Creo firmemente que la inteligencia artificial no nos reemplazar√° todav√≠a. Pero nos asistir√° en forma de asistentes donde los humanos siguen siendo los expertos en control.

## Mis primeros proyectos con LLM üöÄ

Comenc√© a escribir un m√≥dulo de b√∫squeda de rutas `ROS` para una competencia rob√≥tica, generar caracter√≠sticas para una aplicaci√≥n multiplataforma `Flutter` de arquitectura limpia, y cre√© una peque√±a aplicaci√≥n web para rastrear mis gastos en `Next.js`. El hecho de que construyera esta peque√±a aplicaci√≥n en una noche, en un marco que nunca hab√≠a tocado antes, fue un momento que cambi√≥ el juego para m√≠; los LLM no eran solo herramientas, sino multiplicadores. Desarroll√© `bboxconverter`, un paquete para convertir cajas de l√≠mites, y la lista sigue. Los LLM pueden ayudarlo a aprender nuevas tecnolog√≠as y marcos r√°pidamente; eso es genial.

## Un nuevo paradigma: Software 3.0 üí°

Me sumerg√≠ m√°s en los LLM y comenc√© a construir agentes y andamiaje alrededor de ellos. Reproduje el famoso art√≠culo [RestGPT](https://restgpt.github.io/). La idea es excelente: dar a los LLM la capacidad de llamar a algunas API REST con una especificaci√≥n OpenAPI, como `Spotify` o `TMDB`. Estas capacidades introducen un nuevo paradigma de programaci√≥n de software, que me gusta llamar **Software 3.0**.

| Software 1.0     | Software 2.0        | Software 3.0 |
| ---------------- | ------------------- | ------------ |
| Basado en reglas | Impulsado por datos | Agente       |

La misma idea impuls√≥ el protocolo [MCP](https://modelcontextprotocol.io/introduction), que permite a los LLM llamar a herramientas y recursos directamente de manera fluida porque, por dise√±o, la herramienta necesita una descripci√≥n para ser llamada por el LLM en el opuesto de las API REST que no requieren necesariamente una especificaci√≥n OpenAPI.

## Las limitaciones de los LLM üß©

### Alucinaciones üåÄ

Mientras reproduc√≠a el famoso art√≠culo `RESTGPT`, not√© algunas limitaciones graves de los LLM. Los autores del art√≠culo encontraron los mismos problemas que yo: los LLM estaban **alucinando**. Generan c√≥digo que no se implementa, inventando argumentos y simplemente siguiendo las instrucciones al pie de la letra sin aprovechar el sentido com√∫n. Por ejemplo, en el c√≥digo base original de RestGPT, los autores preguntaron en [la indicaci√≥n del llamador](https://github.com/Yifan-Song793/RestGPT/blob/main/model/caller.py).

> "para no ser astuto y hacer pasos que no existen en el plan".

Encontr√© esta afirmaci√≥n divertida y muy interesante porque fue la primera vez que encontr√© a alguien instruyendo a los LLM para que no alucinaran.

### Tama√±o de contexto limitado üìè

Otra limitaci√≥n fue el tama√±o del contexto; los LLM se desempe√±an bien para encontrar la aguja en el pajar, pero luchan por entenderlo. Cuando le das demasiado contexto a los modelos de lenguaje, tienden a perderse en los detalles y perder de vista la imagen general, lo cual es molesto y requiere una direcci√≥n constante. La forma en que me gusta pensar al respecto es de manera similar a [la maldici√≥n de la dimensionalidad](https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb/). Reemplaza la palabra "dimensi√≥n" o "caracter√≠stica" por "contexto", y obtienes la idea.

![Maldici√≥n de la dimensionalidad](/assets/blog/post1/curse_of_dimensionality.png)

Cuanto m√°s contexto le das al LLM, m√°s dif√≠cil es encontrar la respuesta correcta. Surg√≠ con una frase agradable para resumir esta idea:

> Proporcionar la menor cantidad de contexto posible pero la necesaria

Esto est√° fuertemente inspirado en la famosa [cita de Alain Berset](https://www.lematin.ch/story/alain-berset-la-formule-qui-defie-le-temps-166189802108), un pol√≠tico suizo üá®üá≠ que dijo durante el bloqueo de COVID-19:

> "Queremos actuar lo m√°s r√°pido posible, pero tambi√©n lo m√°s lentamente necesario".

Esto representa la idea de compromiso y se aplica al tama√±o del contexto de los LLM.

## Buscando una mejor manera: code2prompt üî®

Por lo tanto, necesitaba una forma de cargar, filtrar y organizar mi contexto de c√≥digo r√°pidamente proporcionando la menor cantidad posible de contexto con la mejor calidad posible. Intent√© copiar manualmente archivos o fragmentos en indicaciones, pero eso se volvi√≥ engorroso y propenso a errores. Sab√≠a que automatizar el proceso tedioso de forjar el contexto para hacer mejores indicaciones ser√≠a √∫til. Luego, un d√≠a, escrib√≠ "code2prompt" en Google, esperando encontrar una herramienta que canalizara mi c√≥digo directamente en indicaciones.

Y he aqu√≠, descubr√≠ un proyecto **basado en Rust** de [Mufeed](https://www.reddit.com/r/rust/comments/1bghroh/i_made_code2prompt_a_cli_tool_to_convert_your/) llamado _code2prompt_, con alrededor de 200 estrellas en GitHub. Todav√≠a era b√°sico en ese momento: una herramienta CLI simple con capacidad de filtro limitada y plantillas. Vi un enorme potencial y me un√≠ directamente para contribuir, implementando la coincidencia de patrones glob, entre otras caracter√≠sticas, y pronto me convert√≠ en el principal contribuyente.

## Visi√≥n e integraciones üîÆ

Hoy en d√≠a, hay varias formas de proporcionar contexto a los LLM. Generar a partir del contexto m√°s grande, utilizando la generaci√≥n aumentada de recuperaci√≥n (RAG), [comprimiendo el c√≥digo](https://www.all-hands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents), o incluso utilizando una combinaci√≥n de estos m√©todos. La creaci√≥n de contexto es un tema candente que evolucionar√° r√°pidamente en los pr√≥ximos meses. Sin embargo, mi enfoque es **KISS**: Mant√©nlo simple, est√∫pido. La mejor forma de proporcionar contexto a los LLM es utilizar la forma m√°s simple y eficiente posible. Forjas precisamente el contexto que necesitas; es determinista, a diferencia de RAG.

Es por eso que decid√≠ impulsar `code2prompt` m√°s lejos como una herramienta simple que se puede utilizar en cualquier flujo de trabajo. Quer√≠a hacerlo f√°cil de usar, f√°cil de integrar y f√°cil de extender. Es por eso que agregu√© nuevas formas de interactuar con la herramienta.

- **N√∫cleo**: El n√∫cleo de `code2prompt` es una biblioteca de Rust que proporciona la funcionalidad b√°sica para forjar contexto a partir de tu base de c√≥digo. Incluye una API simple para cargar, filtrar y organizar tu contexto de c√≥digo.
- **CLI:** La interfaz de l√≠nea de comandos es la forma m√°s simple de usar `code2prompt`. Puedes forjar contexto a partir de tu base de c√≥digo y canalizarlo directamente en tus indicaciones.
- **API de Python:** La API de Python es un envoltorio simple alrededor de la CLI que te permite usar `code2prompt` en tus scripts y agentes de Python. Puedes forjar contexto a partir de tu base de c√≥digo y canalizarlo directamente en tus indicaciones.
- **MCP**: El servidor MCP de `code2prompt` permite a los LLM usar `code2prompt` como una herramienta, lo que les permite ser capaces de forjar el contexto.

La visi√≥n se describe m√°s a fondo en la [p√°gina de visi√≥n](/docs/vision) en el documento.

## Integraci√≥n con agentes üë§

Creo que los agentes futuros necesitar√°n tener una forma de ingerir contexto, y `code2prompt` es la forma simple y eficiente de hacerlo para repositorios textuales como bases de c√≥digo, documentaci√≥n o notas. Un lugar propicio para usar `code2prompt` ser√≠a en una base de c√≥digo con convenciones de nombres significativas. Por ejemplo, en la arquitectura limpia, hay una clara separaci√≥n de preocupaciones y capas. El contexto relevante suele residir en diferentes archivos y carpetas pero comparte el mismo nombre. Este es un caso de uso perfecto para `code2prompt`, donde puedes usar el patr√≥n glob para agarrar los archivos relevantes.

**Basado en patrones glob:** Selecciona o excluye archivos con minimal molestia.

Adem√°s, la biblioteca central est√° dise√±ada como un administrador de contexto estatal, lo que te permite agregar o eliminar archivos a medida que evoluciona tu conversaci√≥n con el LLM. Esto es particularmente √∫til cuando proporcionas contexto para una tarea o objetivo espec√≠fico. Puedes agregar o eliminar archivos del contexto sin volver a ejecutar el proceso.

**Contexto estatal:** Agrega o elimina archivos a medida que evoluciona tu conversaci√≥n con el LLM.

Estas capacidades hacen que `code2prompt` sea un ajuste perfecto para flujos de trabajo basados en agentes. El servidor MCP permite una integraci√≥n perfecta con marcos de agentes de inteligencia artificial populares como [Aider](https://github.com/paul-gauthier/aider), [Goose](https://block.github.io/goose/), o [Cline](https://github.com/jhillyerd/cline). Dejan que manejen objetivos complejos mientras `code2prompt` entrega el contexto de c√≥digo perfecto.

## Por qu√© Code2prompt importa ‚úä

A medida que los LLM evolucionan y las ventanas de contexto se expanden, puede parecer que simplemente forzar a los repositorios enteros en indicaciones es suficiente. Sin embargo, **los costos de tokens** y la **coherencia de las indicaciones** siguen siendo importantes obst√°culos para las peque√±as empresas y los desarrolladores. Centr√°ndose solo en el c√≥digo que importa, `code2prompt` mantiene tu uso de LLM eficiente, rentable y menos propenso a la alucinaci√≥n.

**En resumen:**

- **Reduce las alucinaciones** proporcionando la cantidad adecuada de contexto
- **Reduce los costos de tokens** mediante la curaci√≥n manual del contexto adecuado necesario
- **Mejora el rendimiento de LLM** proporcionando la cantidad adecuada de contexto
- Integra la pila ag√©ntica como un alimentador de contexto para repositorios textuales

## Puedes unirte ¬°Es de c√≥digo abierto! üåê

¬°Todos los nuevos contribuyentes son bienvenidos! ¬°Ven a bordo si est√°s interesado en Rust, forjando herramientas innovadoras de inteligencia artificial o simplemente quieres un mejor flujo de trabajo para tus indicaciones basadas en c√≥digo!

Gracias por leer, y espero que mi historia te haya inspirado a revisar code2prompt. Ha sido un viaje incre√≠ble, y apenas est√° empezando.

**Olivier D'Ancona**

> Esta p√°gina ha sido traducida autom√°ticamente para su conveniencia. Consulte la versi√≥n en ingl√©s para ver el contenido original.
