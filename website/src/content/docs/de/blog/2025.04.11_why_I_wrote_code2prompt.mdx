---
title: "Warum ich Code2Prompt entwickelt habe"
date: 2025-04-11
lastUpdated: 2025-04-11
tags:
  - open-source
  - code2prompt
  - KI
  - Agent
excerpt: "Die Geschichte hinter code2prompt: meine Open-Source-Suche nach L√∂sungen f√ºr Kontext-Herausforderungen in LLM-Workflows"
authors:
  - ODAncona
cover:
  alt: "Eine Illustration von code2prompt, das den Code-Kontext f√ºr KI-Agenten optimiert."
  image: "/src/assets/logo_dark_v0.0.2.svg"
featured: false
draft: false
---

## Einf√ºhrung

Ich bin seit jeher fasziniert davon, wie gro√üe Sprachmodelle (LLMs) die Codierungs-Workflows ver√§ndern - sei es durch die Generierung von Tests, Docstrings oder sogar ganzen Features in Minuten. Aber als ich diese Modelle weiterentwickelte, traten einige kritische Schmerzpunkte auf:

| Planungsprobleme | Hohe Token-Kosten | Halluzinationen |
| ---------------- | ----------------- | --------------- |
| üß† ‚û°Ô∏è ü§Ø         | üî• ‚û°Ô∏è üí∏          | üí¨ ‚û°Ô∏è üåÄ        |

Deshalb begann ich, mich mit `code2prompt` zu besch√§ftigen, einem Rust-basierten Tool, das dabei hilft, den richtigen Kontext f√ºr LLMs bereitzustellen.

In diesem Beitrag teile ich meine Reise und erkl√§re, warum ich davon √ºberzeugt bin, dass `code2prompt` heute relevant ist und sich so gut integrieren l√§sst, und warum es zu meiner bevorzugten L√∂sung f√ºr bessere, schnellere KI-Codierungs-Workflows geworden ist.

## Meine ersten Schritte mit LLMs üë£

Ich begann im November 2023 mit Experimenten mit LLMs auf `OpenAI Playground` mit `text-davinci-003`. Die Sprachmodelle erm√∂glichten eine neue Revolution. Es f√ºhlte sich an, als h√§tte ich einen brillanten neuen Assistenten, der auf Kommando Unit-Tests und Docstrings erstellen konnte. Ich genoss es, die Modelle an ihre Grenzen zu bringen - von Small Talk und ethischen Dilemmata bis hin zu Jailbreaks und komplexen Codierungsaufgaben. Als ich jedoch an umfangreicheren Projekten arbeitete, erkannte ich schnell, dass die Modelle offensichtliche Einschr√§nkungen aufwiesen. Zun√§chst konnte ich nur wenige hundert Zeilen Code in das Kontextfenster einf√ºgen, und selbst dann hatten die Modelle oft Schwierigkeiten, den Zweck oder die Struktur des Codes zu verstehen. Deshalb erkannte ich schnell, dass der Kontext von gr√∂√üter Bedeutung war. Je pr√§ziser meine Anweisungen waren und je besser der Kontext, desto besser die Ergebnisse.

![OpenAI Playground](/assets/blog/post1/playground.png)

## Modell-Evolution üèóÔ∏è

Die Modelle konnten beeindruckende Ergebnisse liefern, aber oft hatten sie Schwierigkeiten mit gr√∂√üeren Codebasen oder komplexen Aufgaben. Ich fand mich immer wieder dabei, mehr Zeit f√ºr die Erstellung von Prompts aufzuwenden als tats√§chlich zu codieren. Gleichzeitig verbesserten sich die Modelle mit der Ver√∂ffentlichung neuer Versionen. Sie erh√∂hten ihre Denkf√§higkeiten und Kontextgr√∂√üe, boten neue Perspektiven und M√∂glichkeiten. Ich konnte fast zweitausend Zeilen Code in das Kontextfenster einf√ºgen, und die Ergebnisse verbesserten sich. Ich konnte ganze Features in wenigen Iterationen schreiben, und ich war beeindruckt davon, wie schnell ich Ergebnisse erzielen konnte. Ich war √ºberzeugt, dass LLMs die Zukunft des Codierens waren, und ich wollte Teil dieser Revolution sein. Ich bin fest davon √ºberzeugt, dass KI uns nicht ersetzen wird, sondern uns als Assistenten unterst√ºtzen wird, w√§hrend Menschen immer noch die Experten sind.

## Meine ersten Projekte mit LLMs üöÄ

Ich begann, ein `ROS`-Pathfinding-Modul f√ºr einen Roboter-Wettbewerb zu schreiben, generierte Features f√ºr eine saubere Architektur-`Flutter`-Cross-Plattform-App und entwickelte eine kleine Web-App, um meine Ausgaben in `Next.js` zu verfolgen. Die Tatsache, dass ich diese kleine App an einem Abend in einem Framework, das ich noch nie zuvor verwendet hatte, erstellte, war ein gamechanger f√ºr mich; LLMs waren nicht nur Werkzeuge, sondern Multiplikatoren. Ich entwickelte `bboxconverter`, ein Paket zum Konvertieren von Bounding-Boxes, und vieles mehr. LLMs k√∂nnen Ihnen helfen, neue Technologien und Frameworks schnell zu erlernen; das ist gro√üartig.

## Ein neues Paradigma: Software 3.0 üí°

Ich tauchte tiefer in LLMs ein und begann, Agenten und Scaffoldings darum herum zu bauen. Ich reproduzierte das ber√ºhmte Paper [RestGPT](https://restgpt.github.io/). Die Idee ist gro√üartig: Geben Sie LLMs die M√∂glichkeit, einige REST-APIs mit einer OpenAPI-Spezifikation aufzurufen, wie z.B. `Spotify` oder `TMDB`. Diese F√§higkeiten f√ºhren ein neues Software-Programmierparadigma ein, das ich **Software 3.0** nenne.

| Software 1.0 | Software 2.0   | Software 3.0 |
| ------------ | -------------- | ------------ |
| Regelbasiert | Datengesteuert | Agentisch    |

Die gleiche Idee trieb das [MCP](https://modelcontextprotocol.io/introduction)-Protokoll voran, das es LLMs erm√∂glicht, Tools und Ressourcen direkt auf eine nahtlose Weise aufzurufen, da das Tool per Design eine Beschreibung ben√∂tigt, um vom LLM aufgerufen zu werden, im Gegensatz zu REST-APIs, die nicht unbedingt eine OpenAPI-Spezifikation erfordern.

## Die Einschr√§nkungen von LLMs üß©

### Halluzinationen üåÄ

W√§hrend ich das ber√ºhmte Paper `RESTGPT` reproduzierte, bemerkte ich einige schwerwiegende Einschr√§nkungen von LLMs. Die Autoren des Papiers begegneten den gleichen Problemen wie ich: LLMs **halluzinierten**. Sie generierten Code, der nicht implementiert war, erfanden Argumente und folgten einfach den Anweisungen buchst√§blich, ohne gesunden Menschenverstand zu verwenden. Zum Beispiel fragten die Autoren in [dem Caller-Prompt](https://github.com/Yifan-Song793/RestGPT/blob/main/model/caller.py).

> "Nicht clever werden und Schritte erfinden, die nicht im Plan existieren."

Ich fand diese Aussage lustig und sehr interessant, weil es das erste Mal war, dass ich jemanden sah, der LLMs anwies, nicht zu halluzinieren.

### Begrenzte Kontextgr√∂√üe üìè

Eine weitere Einschr√§nkung war die Kontextgr√∂√üe; LLMs funktionieren gut beim Finden der Nadel im Heuhaufen, aber haben Schwierigkeiten, einen Sinn daraus zu machen. Wenn Sie den Sprachmodellen zu viel Kontext geben, tendieren sie dazu, sich in den Details zu verlieren und die √úbersicht zu verlieren, was √§rgerlich ist und st√§ndige Steuerung erfordert. Die Art und Weise, wie ich dar√ºber nachdenke, ist √§hnlich wie bei [dem Fluch der Dimensionalit√§t](https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb/). Ersetzen Sie das Wort "Dimension" oder "Feature" durch "Kontext", und Sie erhalten die Idee.

![Fluch der Dimensionalit√§t](/assets/blog/post1/curse_of_dimensionality.png)

Je mehr Kontext Sie dem LLM geben, desto schwieriger ist es, die richtige Antwort zu finden. Ich kam auf einen sch√∂nen Satz, um diese Idee zusammenzufassen:

> Stellen Sie so wenig Kontext wie m√∂glich, aber so viel wie n√∂tig bereit.

Dies ist stark inspiriert von dem ber√ºhmten [Zitat von Alain Berset](https://www.lematin.ch/story/alain-berset-la-formule-qui-defie-le-temps-166189802108), einem Schweizer Politiker üá®üá≠, der w√§hrend des COVID-19-Lockdowns sagte:

> "Wir m√∂chten so schnell wie m√∂glich handeln, aber auch so langsam wie n√∂tig."

Dies repr√§sentiert die Idee des Kompromisses und gilt f√ºr die Kontextgr√∂√üe von LLMs!

## Die Suche nach einem besseren Weg: code2prompt üî®

Deshalb ben√∂tigte ich eine M√∂glichkeit, meinen Code-Kontext schnell zu laden, zu filtern und zu organisieren, indem ich die kleinstm√∂gliche Menge an Kontext mit der besten Qualit√§t bereitstellte. Ich versuchte, Dateien oder Code-Snippets manuell in Prompts zu kopieren, aber das wurde unhandlich und fehleranf√§llig. Ich wusste, dass die Automatisierung des m√ºhsamen Prozesses der Kontextgestaltung, um bessere Prompts zu stellen, hilfreich sein w√ºrde. Dann gab ich eines Tages "code2prompt" in Google ein, in der Hoffnung, ein Tool zu finden, das meinen Code direkt in Prompts einspeist.

Und tats√§chlich entdeckte ich ein **Rust-basiertes Projekt** von [Mufeed](https://www.reddit.com/r/rust/comments/1bghroh/i_made_code2prompt_a_cli_tool_to_convert_your/) namens _code2prompt_, das etwa 200 Sterne auf GitHub hatte. Es war damals noch einfach: ein einfaches CLI-Tool mit grundlegender Filterkapazit√§t und Vorlagen. Ich sah enormes Potenzial und sprang direkt ein, um beizutragen, implementierte unter anderem die glob-Muster√ºbereinstimmung und wurde bald zum Hauptmitarbeiter.

## Vision & Integrationen üîÆ

Heute gibt es mehrere M√∂glichkeiten, Kontext f√ºr LLMs bereitzustellen. Generierung aus dem gr√∂√üeren Kontext, Verwendung von Retrieval-Augmented Generation (RAG), [Komprimierung des Codes](https://www.all-hands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents) oder sogar Verwendung einer Kombination dieser Methoden. Kontextgestaltung ist ein hei√ües Thema, das sich in den kommenden Monaten schnell entwickeln wird. Mein Ansatz ist jedoch **KISS**: Keep It Simple, Stupid. Die beste M√∂glichkeit, Kontext f√ºr LLMs bereitzustellen, besteht darin, die einfachste und effizienteste Methode zu verwenden. Sie gestalten genau den Kontext, den Sie ben√∂tigen; es ist deterministisch, im Gegensatz zu RAG.

Deshalb beschloss ich, `code2prompt` weiterzuentwickeln, als ein einfaches Tool, das in jedem Workflow verwendet werden kann. Ich wollte es einfach zu bedienen, einfach zu integrieren und einfach zu erweitern machen. Deshalb f√ºgte ich neue M√∂glichkeiten hinzu, mit dem Tool zu interagieren.

- **Core**: Der Core von `code2prompt` ist eine Rust-Bibliothek, die die grundlegende Funktionalit√§t bietet, um Kontext aus Ihrem Code-Bestand zu gestalten. Sie enth√§lt eine einfache API, um Ihren Code-Kontext zu laden, zu filtern und zu organisieren.
- **CLI:** Die Kommandozeilen-Schnittstelle ist die einfachste M√∂glichkeit, `code2prompt` zu verwenden. Sie k√∂nnen Kontext aus Ihrem Code-Bestand gestalten und direkt in Ihre Prompts einspeisen.
- **Python-API:** Die Python-API ist eine einfache Wrapper-Funktion um die CLI, die es Ihnen erm√∂glicht, `code2prompt` in Ihren Python-Skripten und Agenten zu verwenden. Sie k√∂nnen Kontext aus Ihrem Code-Bestand gestalten und direkt in Ihre Prompts einspeisen.
- **MCP**: Der `code2prompt`-MCP-Server erm√∂glicht es LLMs, `code2prompt` als Tool zu verwenden, und macht sie dadurch in der Lage, Kontext zu gestalten.

Die Vision wird auf der [Vision-Seite](/docs/vision) im Detail beschrieben.

## Integration mit Agenten üë§

Ich bin davon √ºberzeugt, dass zuk√ºnftige Agenten eine M√∂glichkeit ben√∂tigen, Kontext zu verdauen, und `code2prompt` ist die einfache und effiziente M√∂glichkeit, dies f√ºr Text-Repositorys wie Code-Bestand, Dokumentation oder Notizen zu tun. Ein typischer Ort, an dem `code2prompt` verwendet werden kann, ist in einem Code-Bestand mit sinnvollen Benennungskonventionen. Zum Beispiel gibt es in einer sauberen Architektur eine klare Trennung von Belangen und Schichten. Der relevante Kontext befindet sich normalerweise in verschiedenen Dateien und Ordnern, teilt aber denselben Namen. Dies ist ein perfektes Anwendungsbeispiel f√ºr `code2prompt`, bei dem Sie die glob-Muster√ºbereinstimmung verwenden k√∂nnen, um die relevanten Dateien zu erfassen.

**Glob-Muster zuerst:** W√§hlen Sie Dateien pr√§zise aus oder schlie√üen Sie sie aus mit minimalem Aufwand.

Dar√ºber hinaus ist die Core-Bibliothek als zustandsbehafteter Kontext-Manager konzipiert, der es Ihnen erm√∂glicht, Dateien hinzuzuf√ºgen oder zu entfernen, w√§hrend Ihre Unterhaltung mit dem LLM fortschreitet. Dies ist besonders n√ºtzlich, wenn Sie Kontext f√ºr eine bestimmte Aufgabe oder ein bestimmtes Ziel bereitstellen. Sie k√∂nnen Dateien leicht hinzuf√ºgen oder entfernen, ohne den Prozess neu zu starten.

**Zustandsbehafteter Kontext:** F√ºgen Sie Dateien hinzu oder entfernen Sie sie, w√§hrend Ihre Unterhaltung mit dem LLM fortschreitet.

Diese F√§higkeiten machen `code2prompt` zu einem perfekten Fit f√ºr agentenbasierte Workflows. Der MCP-Server erm√∂glicht eine nahtlose Integration mit beliebten KI-Agenten-Frameworks wie [Aider](https://github.com/paul-gauthier/aider), [Goose](https://block.github.io/goose/) oder [Cline](https://github.com/jhillyerd/cline). Lassen Sie sie komplexe Ziele bearbeiten, w√§hrend `code2prompt` den perfekten Code-Kontext liefert.

## Warum Code2prompt wichtig ist ‚úä

Wenn LLMs sich weiterentwickeln und Kontextfenster expandieren, k√∂nnte es scheinen, als ob das blo√üe Brute-Forcen ganzer Repositorys in Prompts ausreicht. Allerdings bleiben **Token-Kosten** und **Prompt-Koh√§renz** erhebliche Hindernisse f√ºr kleine Unternehmen und Entwickler. Indem Sie sich auf den relevanten Code konzentrieren, h√§lt `code2prompt` Ihre LLM-Nutzung effizient, kosteneffektiv und weniger anf√§llig f√ºr Halluzinationen.

**In K√ºrze:**

- **Reduzieren Sie Halluzinationen**, indem Sie den richtigen Kontext bereitstellen
- **Reduzieren Sie Token-Verbrauchs**-Kosten, indem Sie den richtigen Kontext manuell kuratieren
- **Verbessern Sie die LLM-Leistung**, indem Sie den richtigen Kontext bereitstellen
- Integriert die agentische Stack als Kontext-Feeder f√ºr Text-Repositorys

## Sie k√∂nnen sich anschlie√üen! üåê

Jeder neue Mitwirkende ist willkommen! Kommen Sie an Bord, wenn Sie an Rust, der Gestaltung innovativer KI-Tools oder einfach nur an einem besseren Workflow f√ºr Ihre Code-basierten Prompts interessiert sind.

Vielen Dank f√ºr das Lesen, und ich hoffe, meine Geschichte hat Sie inspiriert, code2prompt zu √ºberpr√ºfen. Es war eine unglaubliche Reise, und sie f√§ngt gerade erst an!

**Olivier D'Ancona**

> Diese Seite wurde f√ºr Ihre Bequemlichkeit automatisch √ºbersetzt. Bitte greifen Sie f√ºr den Originalinhalt auf die englische Version zur√ºck.
